{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Federal Employment Data Analysis\n## Data Exploration, Cleaning & Transformation\n\n**Dataset:** November 2025 Federal Employment Data (~780MB, 2M+ records)\n\n---\n\n### Quick Start\n\n**1. Install Dependencies:**\n```bash\npip install pandas numpy matplotlib seaborn jupyter\n```\n\n**2. Get the Data:**\n- Download the federal employment CSV file\n- Update `RAW_DATA_PATH` in the next cell to point to your file location\n\n**3. Run the Notebook:**\n- Run all cells in order\n- Processed data will be exported to `../data/processed/`\n- View the dashboard by opening `../dashboard/index.html` in a browser\n\n---\n\n### Objectives\n1. Load and profile the data efficiently\n2. Clean and standardize values\n3. Create aggregated datasets for analysis\n4. Export processed data for the interactive dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 50)\npd.set_option('display.float_format', '{:,.2f}'.format)\n\n# Paths\nRAW_DATA_PATH = Path('../data/raw/employment_data.csv')  # Update this path to your data file\nPROCESSED_DATA_PATH = Path('../data/processed')\nPROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n\nprint('Setup complete!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Data Loading\n",
    "\n",
    "The file is ~780MB, so we'll use chunked reading for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's peek at the structure without loading everything\n",
    "df_sample = pd.read_csv(RAW_DATA_PATH, sep='|', nrows=1000)\n",
    "print(f\"Columns ({len(df_sample.columns)}):\")\n",
    "print(df_sample.columns.tolist())\n",
    "print(f\"\\nSample shape: {df_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and memory usage of sample\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total rows without loading full file\n",
    "def count_rows(filepath):\n",
    "    \"\"\"Count rows efficiently without loading entire file\"\"\"\n",
    "    count = 0\n",
    "    for chunk in pd.read_csv(filepath, sep='|', chunksize=100000, usecols=[0]):\n",
    "        count += len(chunk)\n",
    "    return count\n",
    "\n",
    "print(\"Counting total rows (this may take a moment)...\")\n",
    "total_rows = count_rows(RAW_DATA_PATH)\n",
    "print(f\"Total rows in dataset: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Profiling\n",
    "\n",
    "Understanding what's in each column before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile each column - unique values, nulls, data types\n",
    "def profile_columns(df):\n",
    "    \"\"\"Generate a profile of each column\"\"\"\n",
    "    profile = []\n",
    "    for col in df.columns:\n",
    "        profile.append({\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'non_null': df[col].notna().sum(),\n",
    "            'null_count': df[col].isna().sum(),\n",
    "            'null_pct': f\"{df[col].isna().mean()*100:.1f}%\",\n",
    "            'unique': df[col].nunique(),\n",
    "            'sample_values': df[col].dropna().head(3).tolist()\n",
    "        })\n",
    "    return pd.DataFrame(profile)\n",
    "\n",
    "profile_df = profile_columns(df_sample)\n",
    "profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for REDACTED values across columns\n",
    "def check_redacted(df):\n",
    "    \"\"\"Count REDACTED values in each column\"\"\"\n",
    "    redacted_counts = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            redacted_count = (df[col] == 'REDACTED').sum()\n",
    "            if redacted_count > 0:\n",
    "                redacted_counts[col] = redacted_count\n",
    "    return redacted_counts\n",
    "\n",
    "redacted = check_redacted(df_sample)\n",
    "print(\"Columns with REDACTED values:\")\n",
    "for col, count in redacted.items():\n",
    "    print(f\"  {col}: {count} ({count/len(df_sample)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values for key categorical columns\n",
    "categorical_cols = [\n",
    "    'age_bracket', 'agency', 'appointment_type', 'education_level',\n",
    "    'pay_plan', 'supervisory_status', 'work_schedule', 'stem_occupation'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_sample.columns:\n",
    "        print(f\"\\n=== {col} ===\")\n",
    "        print(df_sample[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Full Dataset\n",
    "\n",
    "Now that we understand the structure, let's load the full dataset with optimized dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimized dtypes to reduce memory usage\n",
    "dtype_map = {\n",
    "    'age_bracket': 'category',\n",
    "    'agency': 'category',\n",
    "    'agency_code': 'category',\n",
    "    'agency_subelement': 'category',\n",
    "    'agency_subelement_code': 'category',\n",
    "    'annualized_adjusted_basic_pay': 'object',  # Keep as object due to REDACTED\n",
    "    'appointment_type': 'category',\n",
    "    'appointment_type_code': 'category',\n",
    "    'count': 'int32',\n",
    "    'duty_station_country': 'category',\n",
    "    'duty_station_country_code': 'category',\n",
    "    'duty_station_state': 'category',\n",
    "    'duty_station_state_abbreviation': 'category',\n",
    "    'duty_station_state_code': 'category',\n",
    "    'education_level': 'category',\n",
    "    'education_level_code': 'category',\n",
    "    'grade': 'object',  # Mixed types possible\n",
    "    'length_of_service_years': 'float32',\n",
    "    'occupational_group': 'category',\n",
    "    'occupational_group_code': 'category',\n",
    "    'occupational_series': 'category',\n",
    "    'occupational_series_code': 'category',\n",
    "    'pay_plan': 'category',\n",
    "    'pay_plan_code': 'category',\n",
    "    'snapshot_yyyymm': 'int32',\n",
    "    'stem_occupation': 'category',\n",
    "    'stem_occupation_type': 'category',\n",
    "    'supervisory_status': 'category',\n",
    "    'supervisory_status_code': 'category',\n",
    "    'work_schedule': 'category',\n",
    "    'work_schedule_code': 'category'\n",
    "}\n",
    "\n",
    "print(\"Loading full dataset with optimized dtypes...\")\n",
    "print(\"This may take a few moments for a 780MB file...\")\n",
    "\n",
    "df = pd.read_csv(RAW_DATA_PATH, sep='|', dtype=dtype_map, low_memory=False)\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} rows and {len(df.columns)} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Convert pay to numeric (REDACTED becomes NaN)\n",
    "df_clean['pay_numeric'] = pd.to_numeric(\n",
    "    df_clean['annualized_adjusted_basic_pay'], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Flag redacted records\n",
    "df_clean['is_redacted'] = df_clean['annualized_adjusted_basic_pay'] == 'REDACTED'\n",
    "\n",
    "print(f\"Records with salary data: {df_clean['pay_numeric'].notna().sum():,}\")\n",
    "print(f\"Records with REDACTED salary: {df_clean['is_redacted'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean grade column - extract numeric where possible\n",
    "df_clean['grade_numeric'] = pd.to_numeric(df_clean['grade'], errors='coerce')\n",
    "\n",
    "print(\"Grade distribution (where numeric):\")\n",
    "print(df_clean['grade_numeric'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tenure categories\n",
    "def categorize_tenure(years):\n",
    "    if pd.isna(years):\n",
    "        return 'Unknown'\n",
    "    elif years < 1:\n",
    "        return '< 1 year'\n",
    "    elif years < 5:\n",
    "        return '1-5 years'\n",
    "    elif years < 10:\n",
    "        return '5-10 years'\n",
    "    elif years < 20:\n",
    "        return '10-20 years'\n",
    "    elif years < 30:\n",
    "        return '20-30 years'\n",
    "    else:\n",
    "        return '30+ years'\n",
    "\n",
    "df_clean['tenure_category'] = df_clean['length_of_service_years'].apply(categorize_tenure)\n",
    "df_clean['tenure_category'] = df_clean['tenure_category'].astype('category')\n",
    "\n",
    "print(\"Tenure distribution:\")\n",
    "print(df_clean['tenure_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pay bands\n",
    "def categorize_pay(pay):\n",
    "    if pd.isna(pay):\n",
    "        return 'Unknown/Redacted'\n",
    "    elif pay < 40000:\n",
    "        return '< $40K'\n",
    "    elif pay < 60000:\n",
    "        return '$40K-$60K'\n",
    "    elif pay < 80000:\n",
    "        return '$60K-$80K'\n",
    "    elif pay < 100000:\n",
    "        return '$80K-$100K'\n",
    "    elif pay < 150000:\n",
    "        return '$100K-$150K'\n",
    "    elif pay < 200000:\n",
    "        return '$150K-$200K'\n",
    "    else:\n",
    "        return '$200K+'\n",
    "\n",
    "df_clean['pay_band'] = df_clean['pay_numeric'].apply(categorize_pay)\n",
    "df_clean['pay_band'] = df_clean['pay_band'].astype('category')\n",
    "\n",
    "print(\"Pay band distribution:\")\n",
    "print(df_clean['pay_band'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agency overview\n",
    "agency_summary = df_clean.groupby('agency').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': ['mean', 'median', 'min', 'max'],\n",
    "    'length_of_service_years': 'mean',\n",
    "    'grade_numeric': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "agency_summary.columns = ['employee_count', 'avg_pay', 'median_pay', 'min_pay', 'max_pay', 'avg_tenure', 'avg_grade']\n",
    "agency_summary = agency_summary.sort_values('employee_count', ascending=False)\n",
    "\n",
    "print(\"Top 20 Agencies by Employee Count:\")\n",
    "agency_summary.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top agencies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Employee count\n",
    "top_agencies = agency_summary.head(15)\n",
    "axes[0].barh(range(len(top_agencies)), top_agencies['employee_count'])\n",
    "axes[0].set_yticks(range(len(top_agencies)))\n",
    "axes[0].set_yticklabels(top_agencies.index)\n",
    "axes[0].set_xlabel('Employee Count')\n",
    "axes[0].set_title('Top 15 Agencies by Headcount')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Average pay\n",
    "axes[1].barh(range(len(top_agencies)), top_agencies['avg_pay'])\n",
    "axes[1].set_yticks(range(len(top_agencies)))\n",
    "axes[1].set_yticklabels(top_agencies.index)\n",
    "axes[1].set_xlabel('Average Pay ($)')\n",
    "axes[1].set_title('Average Pay by Agency')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/agency_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay by education level\n",
    "education_pay = df_clean.groupby('education_level').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': ['mean', 'median']\n",
    "}).round(2)\n",
    "education_pay.columns = ['employee_count', 'avg_pay', 'median_pay']\n",
    "education_pay = education_pay.sort_values('median_pay', ascending=False)\n",
    "\n",
    "print(\"Pay by Education Level:\")\n",
    "education_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution\n",
    "state_summary = df_clean.groupby('duty_station_state').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean'\n",
    "}).round(2)\n",
    "state_summary.columns = ['employee_count', 'avg_pay']\n",
    "state_summary = state_summary.sort_values('employee_count', ascending=False)\n",
    "\n",
    "print(\"Top 20 States by Federal Employment:\")\n",
    "state_summary.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appointment type analysis\n",
    "appointment_summary = df_clean.groupby('appointment_type').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean',\n",
    "    'length_of_service_years': 'mean'\n",
    "}).round(2)\n",
    "appointment_summary.columns = ['employee_count', 'avg_pay', 'avg_tenure']\n",
    "appointment_summary = appointment_summary.sort_values('employee_count', ascending=False)\n",
    "\n",
    "print(\"Employment by Appointment Type:\")\n",
    "appointment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisory status breakdown\n",
    "supervisor_summary = df_clean.groupby('supervisory_status').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean',\n",
    "    'length_of_service_years': 'mean'\n",
    "}).round(2)\n",
    "supervisor_summary.columns = ['employee_count', 'avg_pay', 'avg_tenure']\n",
    "\n",
    "print(\"Supervisory Status Breakdown:\")\n",
    "supervisor_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEM vs Non-STEM comparison\n",
    "stem_summary = df_clean.groupby('stem_occupation').agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean',\n",
    "    'length_of_service_years': 'mean',\n",
    "    'grade_numeric': 'mean'\n",
    "}).round(2)\n",
    "stem_summary.columns = ['employee_count', 'avg_pay', 'avg_tenure', 'avg_grade']\n",
    "\n",
    "print(\"STEM vs Non-STEM Comparison:\")\n",
    "stem_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Tabulation Analysis\n",
    "\n",
    "Compare any columns against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create comparison tables\n",
    "def compare_columns(df, col1, col2, metric='count', agg_col='count'):\n",
    "    \"\"\"\n",
    "    Create a cross-tabulation comparing two columns.\n",
    "    metric: 'count', 'sum', 'mean'\n",
    "    agg_col: column to aggregate (for sum/mean)\n",
    "    \"\"\"\n",
    "    if metric == 'count':\n",
    "        return pd.crosstab(df[col1], df[col2], values=df['count'], aggfunc='sum', margins=True)\n",
    "    elif metric == 'mean':\n",
    "        pivot = df.pivot_table(values=agg_col, index=col1, columns=col2, aggfunc='mean')\n",
    "        return pivot.round(2)\n",
    "    elif metric == 'sum':\n",
    "        pivot = df.pivot_table(values=agg_col, index=col1, columns=col2, aggfunc='sum')\n",
    "        return pivot\n",
    "\n",
    "# Example: Age bracket vs Education level (employee counts)\n",
    "print(\"Employee Count: Age Bracket vs Education Level\")\n",
    "compare_columns(df_clean, 'age_bracket', 'education_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average pay: Agency vs Supervisory Status\n",
    "print(\"Average Pay: Top Agencies vs Supervisory Status\")\n",
    "top_10_agencies = agency_summary.head(10).index.tolist()\n",
    "df_top_agencies = df_clean[df_clean['agency'].isin(top_10_agencies)]\n",
    "compare_columns(df_top_agencies, 'agency', 'supervisory_status', metric='mean', agg_col='pay_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure by Age Bracket and STEM status\n",
    "print(\"Average Tenure: Age Bracket vs STEM Status\")\n",
    "compare_columns(df_clean, 'age_bracket', 'stem_occupation', metric='mean', agg_col='length_of_service_years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Aggregated Data for Dashboard\n",
    "\n",
    "Create smaller, pre-aggregated datasets that the HTML dashboard can load quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 1: Agency summary\n",
    "agency_export = df_clean.groupby(['agency', 'agency_code']).agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': ['mean', 'median', 'std'],\n",
    "    'length_of_service_years': ['mean', 'median'],\n",
    "    'grade_numeric': 'mean',\n",
    "    'is_redacted': 'sum'\n",
    "}).round(2)\n",
    "agency_export.columns = ['_'.join(col).strip('_') for col in agency_export.columns]\n",
    "agency_export = agency_export.reset_index()\n",
    "agency_export.to_csv(PROCESSED_DATA_PATH / 'agency_summary.csv', index=False)\n",
    "print(f\"Exported agency_summary.csv: {len(agency_export)} agencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 2: State summary\n",
    "state_export = df_clean.groupby(['duty_station_state', 'duty_station_state_abbreviation']).agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': ['mean', 'median'],\n",
    "    'length_of_service_years': 'mean'\n",
    "}).round(2)\n",
    "state_export.columns = ['_'.join(col).strip('_') for col in state_export.columns]\n",
    "state_export = state_export.reset_index()\n",
    "state_export.to_csv(PROCESSED_DATA_PATH / 'state_summary.csv', index=False)\n",
    "print(f\"Exported state_summary.csv: {len(state_export)} states/territories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 3: Occupation summary\n",
    "occupation_export = df_clean.groupby(['occupational_group', 'occupational_series', 'stem_occupation']).agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': ['mean', 'median'],\n",
    "    'length_of_service_years': 'mean',\n",
    "    'grade_numeric': 'mean'\n",
    "}).round(2)\n",
    "occupation_export.columns = ['_'.join(col).strip('_') for col in occupation_export.columns]\n",
    "occupation_export = occupation_export.reset_index()\n",
    "occupation_export.to_csv(PROCESSED_DATA_PATH / 'occupation_summary.csv', index=False)\n",
    "print(f\"Exported occupation_summary.csv: {len(occupation_export)} occupation series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 4: Demographics summary (for charts)\n",
    "demographics_export = df_clean.groupby(['age_bracket', 'education_level', 'tenure_category']).agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean'\n",
    "}).round(2)\n",
    "demographics_export.columns = ['employee_count', 'avg_pay']\n",
    "demographics_export = demographics_export.reset_index()\n",
    "demographics_export.to_csv(PROCESSED_DATA_PATH / 'demographics_summary.csv', index=False)\n",
    "print(f\"Exported demographics_summary.csv: {len(demographics_export)} demographic combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 5: Pay distribution data\n",
    "pay_distribution = df_clean.groupby(['pay_band', 'agency']).agg({\n",
    "    'count': 'sum'\n",
    "}).reset_index()\n",
    "pay_distribution.to_csv(PROCESSED_DATA_PATH / 'pay_distribution.csv', index=False)\n",
    "print(f\"Exported pay_distribution.csv: {len(pay_distribution)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 6: Appointment type summary\n",
    "appointment_export = df_clean.groupby(['appointment_type', 'agency']).agg({\n",
    "    'count': 'sum',\n",
    "    'pay_numeric': 'mean',\n",
    "    'length_of_service_years': 'mean'\n",
    "}).round(2)\n",
    "appointment_export.columns = ['employee_count', 'avg_pay', 'avg_tenure']\n",
    "appointment_export = appointment_export.reset_index()\n",
    "appointment_export.to_csv(PROCESSED_DATA_PATH / 'appointment_summary.csv', index=False)\n",
    "print(f\"Exported appointment_summary.csv: {len(appointment_export)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export 7: Overall statistics (for dashboard header)\n",
    "overall_stats = {\n",
    "    'total_employees': int(df_clean['count'].sum()),\n",
    "    'total_agencies': int(df_clean['agency'].nunique()),\n",
    "    'total_states': int(df_clean['duty_station_state'].nunique()),\n",
    "    'avg_salary': round(df_clean['pay_numeric'].mean(), 2),\n",
    "    'median_salary': round(df_clean['pay_numeric'].median(), 2),\n",
    "    'avg_tenure': round(df_clean['length_of_service_years'].mean(), 2),\n",
    "    'pct_redacted': round(df_clean['is_redacted'].mean() * 100, 2),\n",
    "    'snapshot_date': int(df_clean['snapshot_yyyymm'].iloc[0])\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(PROCESSED_DATA_PATH / 'overall_stats.json', 'w') as f:\n",
    "    json.dump(overall_stats, f, indent=2)\n",
    "    \n",
    "print(\"\\nOverall Statistics:\")\n",
    "for k, v in overall_stats.items():\n",
    "    print(f\"  {k}: {v:,}\" if isinstance(v, int) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all exported files\n",
    "print(\"\\n=== Exported Files ===\")\n",
    "for f in sorted(PROCESSED_DATA_PATH.glob('*')):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name}: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis Functions\n",
    "\n",
    "Reusable functions for Jon to compare any columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_column(df, group_col, sort_by='employee_count', top_n=20):\n",
    "    \"\"\"\n",
    "    Quick analysis of any categorical column.\n",
    "    Returns employee count, average pay, and tenure.\n",
    "    \"\"\"\n",
    "    result = df.groupby(group_col).agg({\n",
    "        'count': 'sum',\n",
    "        'pay_numeric': ['mean', 'median'],\n",
    "        'length_of_service_years': 'mean',\n",
    "        'grade_numeric': 'mean'\n",
    "    }).round(2)\n",
    "    result.columns = ['employee_count', 'avg_pay', 'median_pay', 'avg_tenure', 'avg_grade']\n",
    "    result = result.sort_values(sort_by, ascending=False).head(top_n)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(\"Top 15 Occupational Series by Employee Count:\")\n",
    "analyze_by_column(df_clean, 'occupational_series', top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_groups(df, col, group1, group2):\n",
    "    \"\"\"\n",
    "    Compare statistics between two specific values of a column.\n",
    "    \"\"\"\n",
    "    g1 = df[df[col] == group1]\n",
    "    g2 = df[df[col] == group2]\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': ['Employee Count', 'Avg Pay', 'Median Pay', 'Avg Tenure', 'Avg Grade'],\n",
    "        group1: [\n",
    "            g1['count'].sum(),\n",
    "            g1['pay_numeric'].mean(),\n",
    "            g1['pay_numeric'].median(),\n",
    "            g1['length_of_service_years'].mean(),\n",
    "            g1['grade_numeric'].mean()\n",
    "        ],\n",
    "        group2: [\n",
    "            g2['count'].sum(),\n",
    "            g2['pay_numeric'].mean(),\n",
    "            g2['pay_numeric'].median(),\n",
    "            g2['length_of_service_years'].mean(),\n",
    "            g2['grade_numeric'].mean()\n",
    "        ]\n",
    "    }).round(2)\n",
    "    \n",
    "    comparison['Difference'] = comparison[group2] - comparison[group1]\n",
    "    return comparison\n",
    "\n",
    "# Example: Compare STEM vs non-STEM\n",
    "print(\"STEM vs Non-STEM Occupations Comparison:\")\n",
    "compare_two_groups(df_clean, 'stem_occupation', 'ALL OTHER OCCUPATIONS', 'STEM OCCUPATIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_analyze(df, **filters):\n",
    "    \"\"\"\n",
    "    Filter data by multiple criteria and return summary.\n",
    "    \n",
    "    Example:\n",
    "        filter_and_analyze(df, agency='DEPARTMENT OF TREASURY', education_level='MASTER\\'S DEGREE')\n",
    "    \"\"\"\n",
    "    filtered = df.copy()\n",
    "    for col, value in filters.items():\n",
    "        if col in filtered.columns:\n",
    "            filtered = filtered[filtered[col] == value]\n",
    "    \n",
    "    if len(filtered) == 0:\n",
    "        return \"No records match the specified filters.\"\n",
    "    \n",
    "    summary = {\n",
    "        'Records': len(filtered),\n",
    "        'Employee Count': filtered['count'].sum(),\n",
    "        'Avg Pay': filtered['pay_numeric'].mean(),\n",
    "        'Median Pay': filtered['pay_numeric'].median(),\n",
    "        'Avg Tenure': filtered['length_of_service_years'].mean(),\n",
    "        'Avg Grade': filtered['grade_numeric'].mean()\n",
    "    }\n",
    "    return pd.Series(summary).round(2)\n",
    "\n",
    "# Example: Treasury employees with Master's degree\n",
    "print(\"Treasury Department - Master's Degree Holders:\")\n",
    "filter_and_analyze(df_clean, agency='DEPARTMENT OF TREASURY', education_level=\"MASTER'S DEGREE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The processed data files are now ready in `../data/processed/`:\n",
    "\n",
    "1. **agency_summary.csv** - Per-agency statistics\n",
    "2. **state_summary.csv** - Per-state statistics  \n",
    "3. **occupation_summary.csv** - Per-occupation statistics\n",
    "4. **demographics_summary.csv** - Age/education/tenure breakdowns\n",
    "5. **pay_distribution.csv** - Pay bands by agency\n",
    "6. **appointment_summary.csv** - Appointment types by agency\n",
    "7. **overall_stats.json** - High-level dashboard metrics\n",
    "\n",
    "These files will power the interactive HTML dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}